{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9e445a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a223fc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['username', 'comment', 'time', 'likes', 'sentimen', 'Unnamed: 5'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Ganti path ke lokasi file Anda\n",
    "# Gunakan file dan kolom yang sesuai dengan komentar_vo3 _labeling.csv\n",
    "file_path = 'komentar_vo3 _labeling.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Pastikan kolom yang diperlukan ada\n",
    "print(df.columns)\n",
    "# Gunakan kolom 'comment', 'likes', 'sentimen'\n",
    "df = df[['comment', 'likes', 'sentimen']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "549c97a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed and augmented data saved to preprocessed_comments.csv. Total samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing & Data Augmentation to 10,000 samples\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "stop_words = set(stopwords.words('indonesian'))\n",
    "\n",
    "# Small synonym dictionary for Indonesian\n",
    "synonym_dict = {\n",
    "    'bagus': ['baik', 'hebat'],\n",
    "    'buruk': ['jelek', 'tidak baik'],\n",
    "    'cepat': ['lekas', 'segera'],\n",
    "    'lambat': ['pelan', 'lelet'],\n",
    "    'senang': ['bahagia', 'gembira'],\n",
    "    'sedih': ['duka', 'murung'],\n",
    "    'keren': ['hebat', 'mantap'],\n",
    "    'jelek': ['buruk', 'tidak bagus'],\n",
    "    'suka': ['gemar', 'senang'],\n",
    "    'tidak': ['nggak', 'tak'],\n",
    "    'bagusnya': ['baiknya', 'hebatnya'],\n",
    "    'mantap': ['keren', 'hebat'],\n",
    "    'hebat': ['keren', 'mantap'],\n",
    "    'parah': ['buruk', 'jelek'],\n",
    "    'menarik': ['seru', 'asik'],\n",
    "    'asik': ['seru', 'menarik'],\n",
    "    'seru': ['asik', 'menarik'],\n",
    "}\n",
    "\n",
    "import random\n",
    "\n",
    "def synonym_replacement(tokens):\n",
    "    new_tokens = tokens.copy()\n",
    "    for i, t in enumerate(new_tokens):\n",
    "        if t in synonym_dict and random.random() < 0.3:\n",
    "            new_tokens[i] = random.choice(synonym_dict[t])\n",
    "    return new_tokens\n",
    "\n",
    "def random_swap(tokens):\n",
    "    new_tokens = tokens.copy()\n",
    "    if len(new_tokens) > 1 and random.random() < 0.5:\n",
    "        idx1, idx2 = random.sample(range(len(new_tokens)), 2)\n",
    "        new_tokens[idx1], new_tokens[idx2] = new_tokens[idx2], new_tokens[idx1]\n",
    "    return new_tokens\n",
    "\n",
    "def random_deletion(tokens, p=0.1):\n",
    "    if len(tokens) == 1:\n",
    "        return tokens\n",
    "    return [t for t in tokens if random.random() > p or t in stop_words]\n",
    "\n",
    "def augment_text(text):\n",
    "    tokens = text.split()\n",
    "    aug_methods = [synonym_replacement, random_swap, random_deletion]\n",
    "    random.shuffle(aug_methods)\n",
    "    for method in aug_methods:\n",
    "        tokens = method(tokens)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+|\\#\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip()\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    text = ' '.join(tokens)\n",
    "    return stemmer.stem(text)\n",
    "\n",
    "# Clean original data\n",
    "original_df = df.copy()\n",
    "original_df['clean_text'] = original_df['comment'].apply(preprocessing)\n",
    "\n",
    "# Augment data to reach 10,000 samples\n",
    "augmented_rows = []\n",
    "current_count = len(original_df)\n",
    "if current_count < 10000:\n",
    "    needed = 10000 - current_count\n",
    "    base_rows = original_df[['comment', 'likes', 'sentimen', 'clean_text']].values.tolist()\n",
    "    for _ in range(needed):\n",
    "        base = random.choice(base_rows)\n",
    "        # Augment the clean_text\n",
    "        aug_text = augment_text(base[3])\n",
    "        augmented_rows.append({\n",
    "            'comment': base[0],\n",
    "            'likes': base[1],\n",
    "            'sentimen': base[2],\n",
    "            'clean_text': aug_text\n",
    "        })\n",
    "    aug_df = pd.DataFrame(augmented_rows)\n",
    "    final_df = pd.concat([original_df, aug_df], ignore_index=True)\n",
    "else:\n",
    "    final_df = original_df\n",
    "\n",
    "final_df.to_csv('preprocessed_comments.csv', index=False)\n",
    "print(f\"Preprocessed and augmented data saved to preprocessed_comments.csv. Total samples: {len(final_df)}\")\n",
    "df = final_df  # update df for next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d917060f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix saved to tfidf_matrix.csv\n",
      "TF-IDF + likes features saved to tfidf_plus_likes.csv\n",
      "TF-IDF + likes features saved to tfidf_plus_likes.csv\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF with n-gram, min_df, max_df\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), min_df=5, max_df=0.85)\n",
    "X_text = tfidf.fit_transform(df['clean_text'])\n",
    "tfidf_df = pd.DataFrame(X_text.toarray(), columns=tfidf.get_feature_names_out())\n",
    "tfidf_df.to_csv('tfidf_matrix.csv', index=False)\n",
    "print(\"TF-IDF matrix saved to tfidf_matrix.csv\")\n",
    "\n",
    "# Gabungkan dengan fitur numerik (likes)\n",
    "X_extra = df[['likes']].fillna(0).values\n",
    "from scipy.sparse import hstack\n",
    "X = hstack([X_text, X_extra])\n",
    "combined_df = tfidf_df.copy()\n",
    "combined_df['likes'] = X_extra\n",
    "combined_df.to_csv('tfidf_plus_likes.csv', index=False)\n",
    "print(\"TF-IDF + likes features saved to tfidf_plus_likes.csv\")\n",
    "\n",
    "y = df['sentimen']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae0cdfb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Best score: 0.974\n"
     ]
    }
   ],
   "source": [
    "# Model SVM OVO + GridSearchCV for C, kernel, gamma\n",
    "from sklearn.utils import check_X_y\n",
    "if hasattr(X, \"toarray\"):\n",
    "    X_dense = X.toarray()\n",
    "else:\n",
    "    X_dense = X\n",
    "X_dense = np.nan_to_num(X_dense)\n",
    "y = pd.to_numeric(y, errors='coerce').fillna(0).astype(int)\n",
    "X_dense, y = check_X_y(X_dense, y)\n",
    "\n",
    "svm = SVC(decision_function_shape='ovo')\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "cv_val = min(4, len(y)) if len(y) >= 2 else 2\n",
    "grid = GridSearchCV(svm, param_grid, cv=cv_val, scoring='accuracy', n_jobs=-1)\n",
    "grid.fit(X_dense, y)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best score:\", grid.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98405ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to sentiment_predictions.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.98      0.97      0.97      1859\n",
      "           0       0.97      0.98      0.98      4899\n",
      "           1       0.97      0.97      0.97      3242\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.97      0.97      0.97     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1802   44   13]\n",
      " [  39 4790   70]\n",
      " [   7   87 3148]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi dengan cross_val_predict\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = cross_val_predict(best_model, X, y, cv=4)\n",
    "results_df = df.copy()\n",
    "results_df['predicted_sentimen'] = y_pred\n",
    "results_df.to_csv('sentiment_predictions.csv', index=False)\n",
    "print(\"Predictions saved to sentiment_predictions.csv\")\n",
    "print(classification_report(y, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
